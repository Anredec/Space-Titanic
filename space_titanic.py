# -*- coding: utf-8 -*-
"""Space titanic

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/space-titanic-cb432d59-0550-4047-95b3-f758d5700bb1.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241030/auto/storage/goog4_request%26X-Goog-Date%3D20241030T202246Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D40fcd4288dbb4dd392085e748e34f6060759992e05e52887a2ae2e066ea1372d30ade9a098d1e6c2608d43974e3d9ed439fe0710d13ca0b2e018d10bad74df62257aa8265d6f09af6d6aa575df73fde7eabad4e2cb0ca65cf85a9f1c3286e4f506b60c0794ce41a3b75936a057e64b1527fb6de08363c7a1cb11291a698657e40d530de9e7a11d9331036d515c32a0317d49172e83c05538b7662295473804fa0b0d61c23c243e24323114503bd427d3e4bffa029f027aaac7071a1793f9cb6f04d5cc60808b6c4db560c07ef83423a0314f288252898da5266be43e45ee22c762dd9cb7bb057c3956b8b169c8d6c83d0090f59e0206e408caa3d3b5204629e8
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

spaceship_titanic_path = kagglehub.competition_download('spaceship-titanic')

print('Data source import complete.')

#data processing
import pandas as pd
#linear algebra
import numpy as np
#plotting
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
#algorithm creation
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
#algorithm fitting
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score, roc_curve,roc_auc_score
from sklearn.preprocessing import LabelEncoder
#statistics
import scipy.stats as stats
#suppresing future warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

train_data_original= pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')
test_data_original= pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')

train_data= pd.concat([train_data_original, test_data_original], ignore_index= True)
train_data.head()

"""## 1. Descriptive analysis


"""

train_data.info()

train_data.describe(include= 'all')

"""## 2. Exploratory analysis


### a) Categorical y target Variable

"""

#let's make a simple graph to see how the variables are distributed the variables
categorical_data= train_data[['HomePlanet','CryoSleep','Destination','VIP','Transported']].copy()
fig, ax= plt.subplots(5,1, figsize=(10,10))# constrained_layout= True)
plt.subplots_adjust(top=2)
color= ["b","g","r","c","m","y","k","w"]
#fig.tight_layout()
b=0
for x,y in categorical_data.items():
  ax[b].set_title(x)
  categorical_data[x].value_counts().plot(kind= 'bar', ax= ax[b], color= color[b])
  plt.subplots_adjust(hspace=1)


  b+=1
fig.suptitle('Distribution of categorical Variables ', fontsize=20, fontweight='bold')

"""### b) Categorical separated by target variable"""

categorical_data = train_data[['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Transported']].copy()
categorical_variables = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']

# Create subplots
fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(10, 15))
plt.subplots_adjust(hspace=0.8)

for i, var in enumerate(categorical_variables):
    ax = axes[i]
    counts = categorical_data.groupby([var, 'Transported']).size().unstack(fill_value=0)

    # Plot the bar chart
    counts.plot(kind='bar', ax=ax, color=['b', 'r'], width=0.8)
    ax.set_title(var)
    ax.set_ylabel('Count')

fig.suptitle('Distribution of Categorical Variables - Gruped by Transported', fontsize=20, fontweight='bold')

plt.show()

"""### c) Cuantitative data (expenses)"""

quantitative_data= train_data[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']]

fig, ax2= plt.subplots(1,5, figsize= (15,5))
plt.subplots_adjust(top=0.85)
b=0
for x,y in quantitative_data.items():
  ax2[b].set_title(x)
  quantitative_data[x].plot(kind= 'hist', ax= ax2[b], color= color[b])
  b+=1

quantitative_data = train_data[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck', 'Transported']].copy()
quantitative_variables = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']

# Create subplots: 5 rows, 1 column
fig, axes = plt.subplots(nrows=5, ncols=1, figsize=(10, 15))
plt.subplots_adjust(hspace=0.4)  # Adjust spacing between plots

for i, var in enumerate(quantitative_variables):
    ax = axes[i]  # Select the axis to plot on

    # Plot the histograms for each 'Transported' category
    n, bins, _ = ax.hist(quantitative_data[quantitative_data['Transported'] == 0][var].dropna(),
                         alpha=0.5, label='Not Transported', color='b', bins=20)
    n1, bins1, _= ax.hist(quantitative_data[quantitative_data['Transported'] == 1][var].dropna(),
            alpha=0.5, label='Transported', color='r', bins=20)



    # Adding density curve for 'Transported=1'
    kde_0 = gaussian_kde(quantitative_data[quantitative_data['Transported'] == 0][var].dropna())
    kde_0_x = np.linspace(min(quantitative_data[var].dropna()), max(quantitative_data[var].dropna()), 1000)
    kde_0_y = kde_0(kde_0_x)

    kde_1 = gaussian_kde(quantitative_data[quantitative_data['Transported'] == 1][var].dropna())
    kde_1_x = np.linspace(min(quantitative_data[var].dropna()), max(quantitative_data[var].dropna()), 1000)
    kde_1_y = kde_1(kde_1_x)

    # Scale KDE to match the histogram
    kde_factor = max(n) / max(kde_0_y)  # Scaling factor
    ax2 = ax.twinx()  # Create a secondary y-axis
    ax2.plot(kde_0_x, kde_0_y * kde_factor, color='blue', linewidth=2, label='KDE Transported=0')

    kde_factor1 = max(n1) / max(kde_1_y)  # Scaling factor
    ax2.plot(kde_1_x, kde_1_y * kde_factor1, color='red', linewidth=2, label='KDE Transported=1')

    # Set the x-axis limits to prevent it from exceeding a specific amount
    ax.set_xlim(0,5000)

    # Set the title for each subplot
    ax.set_title(var)

    # Adding labels to axes
    ax.set_xlabel(var)
    ax.set_ylabel('Count')
    ax2.set_ylabel('Density (scaled)')

    # Adding a legend to both histograms and KDE
    ax.legend(loc='upper left')
    ax2.legend(loc='upper right')

# Adjust the overall title for the figure
fig.suptitle('Quantitative Variables - Transported', fontsize=20, fontweight='bold')

plt.show()

"""### d) Quantitative data (Age)"""

age_df = train_data[['Age', 'Transported']].copy()

plt.figure(figsize=(10,6))


n_0, bins_0, _ = plt.hist(age_df[age_df['Transported'] == 0]['Age'].dropna(), alpha=0.50, label="Not Transported", color='r', bins=20, density=False)
n_1, bins_1, _ = plt.hist(age_df[age_df['Transported'] == 1]['Age'].dropna(), alpha=0.50, label="Transported", color='b', bins=20, density=False)

# Adding KDE for 'Not Transported'
kde_nt = gaussian_kde(age_df[age_df['Transported'] == 0]['Age'].dropna())
kde_nt_x = np.linspace(min(age_df['Age'].dropna()), max(age_df['Age'].dropna()), 1000)
kde_nt_y = kde_nt(kde_nt_x)

# Scaling KDE to match histogram
kde_nt_y_scaled = kde_nt_y * max(n_0) / max(kde_nt_y)

# Plot the KDE line
plt.plot(kde_nt_x, kde_nt_y_scaled, color='r', linewidth=2, label="KDE Not Transported")

# Adding KDE for 'Transported'
kde_t = gaussian_kde(age_df[age_df['Transported'] == 1]['Age'].dropna())
kde_t_x = np.linspace(min(age_df['Age'].dropna()), max(age_df['Age'].dropna()), 1000)
kde_t_y = kde_t(kde_t_x)

# Scaling KDE to match histogram
kde_t_y_scaled = kde_t_y * max(n_1) / max(kde_t_y)

# Plot the KDE line
plt.plot(kde_t_x, kde_t_y_scaled, color='b', linewidth=2, label="KDE Transported")

# Labels and legend
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Age Distribution for Transported and Not Transported')
plt.legend()

# Show plot
plt.show()

"""## 3. Data preprocesing"""

#let's separate the cabin column in 3: deck/num/side this way we can  we have a
#more information for the model as before it couldn't give much info
try:
  temp_df= train_data['Cabin'].str.split('/', expand=True)
except:
  train_data= train_data_original
  temp_df= train_data['Cabin'].str.split('/', expand=True)
temp_df.columns= ['Cabin_deck','Cabin_num','Cabin_side']
train_data= pd.concat([train_data, temp_df],axis=1)
train_data.drop(columns=['Cabin'], inplace= True)

train_data

#checking the distribution of the new data created
temp_df= train_data[['Cabin_deck','Cabin_side','Transported']]

fig, ax3= plt.subplots(2,2, figsize= (10,5))
plt.subplots_adjust(top=1.5)
ax3[0,0].set_title("Cabin deck division")
counts1= temp_df.groupby(['Cabin_deck','Transported']).size().unstack(fill_value=1)
counts1.plot(kind='bar',color=['r', 'b'], width=0.8,ax= ax3[0,0])

ax3[0,1].set_title("Cabin deck division")
temp_df['Cabin_deck'].value_counts().plot(kind= 'pie',ax= ax3[0,1], colors= color, autopct= '%1.1f%%')
ax3[1,0].set_title("Cabin side division")
counts1= temp_df.groupby(['Cabin_side','Transported']).size().unstack(fill_value=1)
counts1.plot(kind='bar',color=['r', 'b'], width=0.8,ax= ax3[1,0])
ax3[1,1].set_title("Cabin side division")
temp_df['Cabin_side'].value_counts().plot(kind= 'pie',ax= ax3[1,1], colors= color, autopct= '%1.1f%%')

#now we'll separate the PassengerId in Pgroup, Gsize and if he is a solo traveler (solo_t)
temp_df= train_data
#Pgroup
temp_df['Pgroup']=temp_df['PassengerId'].str.split('_').str[0]

#gsize
group_size = temp_df.groupby('Pgroup').size().reset_index(name='GSize')
temp_df = temp_df.merge(group_size, on='Pgroup')

#solo_t
temp_df['Solo_t'] = temp_df['GSize'].apply(lambda x: 0 if x > 1 else 1)

temp_df.drop(columns=['PassengerId'], inplace= True)


train_data= temp_df
train_data

plt.figure(figsize=(10,5))
counts= temp_df.groupby(['GSize','Transported']).size().unstack(fill_value=1)
counts.plot(kind='bar',color=['r', 'b'], width=0.8)
plt.title("Transported people by group size")
plt.ylabel("Frequency")
plt.xlabel("Group size")
plt.show

plt.figure(figsize=(10,5))
counts= temp_df.groupby(['Solo_t','Transported']).size().unstack(fill_value=1)
counts.plot(kind='bar',color=['r', 'b'], width=0.8)
plt.title("Transported people by solo traveling")
plt.ylabel("Frequency")
plt.xlabel("Solo or not")
plt.show

"""### b) Filling missing data"""

# filling home planet data
mv_hp_raw = train_data['HomePlanet'].isna().sum()

train_data['HomePlanet'] = train_data.groupby('Pgroup')['HomePlanet'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))

mv_hp_first = train_data['HomePlanet'].isna().sum()

print('The number of missing values in HomePlanet: ', mv_hp_raw)
print('The number of missing values in HomePlanet after the first cleaning: ', mv_hp_first)

# LaseName
train_data['LastName'] = train_data['Name'].apply(lambda x: x.split(' ')[1] if not pd.isna(x) else np.nan)

# Second
ln_gb = train_data.groupby('LastName')['HomePlanet'].agg(pd.Series.mode)
ln_index = train_data[train_data['HomePlanet'].isna() & train_data['LastName'].notna() & train_data['LastName'].isin(ln_gb.index)].index

def fill_homeplanet(last_name):
    homeplanet_mode = ln_gb[last_name]
    if isinstance(homeplanet_mode, np.ndarray):
        return homeplanet_mode[0] if homeplanet_mode.size > 0 else np.nan
    return homeplanet_mode

train_data.loc[ln_index, 'HomePlanet'] = train_data.loc[ln_index, 'LastName'].map(fill_homeplanet)

mv_hp_second = train_data['HomePlanet'].isna().sum()

print('The number of missing values in HomePlanet after the second cleaning: ', mv_hp_second)

# filling home planet data
mv_hp_raw = train_data['HomePlanet'].isna().sum()

train_data['HomePlanet'] = train_data.groupby('Pgroup')['HomePlanet'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))

mv_hp_first = train_data['HomePlanet'].isna().sum()

print('The number of missing values in HomePlanet: ', mv_hp_raw)
print('The number of missing values in HomePlanet after the first cleaning: ', mv_hp_first)

train_data['HomePlanet'] = train_data['HomePlanet'].fillna(train_data['HomePlanet'].mode()[0])

mv_hp_third = train_data['HomePlanet'].isna().sum()

print('The number of missing values in HomePlanet after the third cleaning: ', mv_hp_third)

# First
mv_dn_raw = train_data['Destination'].isna().sum()

train_data['Destination'] = train_data.groupby('Pgroup')['Destination'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))

mv_dn_first = train_data['Destination'].isna().sum()

print('The number of missing values in Destination: ', mv_dn_raw)
print('The number of missing values in Destination after the first cleaning: ', mv_dn_first)

# First
mv_dn_raw = train_data['Destination'].isna().sum()

train_data['Destination'] = train_data.groupby('Pgroup')['Destination'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))

mv_dn_first = train_data['Destination'].isna().sum()

print('The number of missing values in Destination: ', mv_dn_raw)
print('The number of missing values in Destination after the first cleaning: ', mv_dn_first)

# First
mv_dn_raw = train_data['Destination'].isna().sum()

train_data['Destination'] = train_data.groupby('Pgroup')['Destination'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))

mv_dn_first = train_data['Destination'].isna().sum()

print('The number of missing values in Destination: ', mv_dn_raw)
print('The number of missing values in Destination after the first cleaning: ', mv_dn_first)

#creating a total expent money variable to fill VIP status or not
expense_df= train_data.copy()

vip_df= expense_df[expense_df['VIP']== True]
novip_df= expense_df[expense_df['VIP']== False]

average_spent_VIP = round(np.mean(vip_df['RoomService'] + vip_df['FoodCourt'] + vip_df['ShoppingMall'] + vip_df['Spa'] + vip_df['VRDeck']), 2)
average_spent_non = round(np.mean(novip_df['RoomService'] + novip_df['FoodCourt'] + novip_df['ShoppingMall'] + novip_df['Spa'] + novip_df['VRDeck']), 2)

print(f"VIPs spent {average_spent_VIP}. On the other hand, non-VIPs spent {average_spent_non}.")

std_VIP=round(np.std(vip_df['RoomService'] + vip_df['FoodCourt'] + vip_df['ShoppingMall'] + vip_df['Spa'] + vip_df['VRDeck']), 2)
se = std_VIP/ vip_df.shape[0]
confidence_level = 0.95
z_value = stats.norm.ppf((1 + confidence_level) / 2)
margin_of_error = round(z_value * se, 2)

print(f"The Margin of Error is {margin_of_error}")
print(f"The lowerbound is {average_spent_VIP - margin_of_error}")

mv_vip_raw = expense_df['VIP'].isna().sum()

expense_df['Expenditure'] = expense_df[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].sum(axis=1)
expense_df.loc[expense_df['VIP'].isna(), 'VIP'] = expense_df['Expenditure'] >= 4772.18

mv_vip_after = expense_df['VIP'].isna().sum()
train_data= expense_df
print('The number of missing values in Destination before: ', mv_vip_raw)
print('The number of missing values in Destination after cleaning: ', mv_vip_after)

#let's fill the expenses data with the mean
mean_VIP_room= round(np.mean(vip_df['RoomService']),2)
mean_VIP_food= round(np.mean(vip_df['FoodCourt']),2)
mean_VIP_Mall= round(np.mean(vip_df['ShoppingMall']),2)
mean_VIP_Spa= round(np.mean(vip_df['Spa']),2)
mean_VIP_VR= round(np.mean(vip_df['VRDeck']),2)

mean_nVIP_room= round(np.mean(novip_df['RoomService']),2)
mean_nVIP_food= round(np.mean(novip_df['FoodCourt']),2)
mean_nVIP_Mall= round(np.mean(novip_df['ShoppingMall']),2)
mean_nVIP_Spa= round(np.mean(novip_df['Spa']),2)
mean_nVIP_VR= round(np.mean(novip_df['VRDeck']),2)

print(f"Vip means: Room service= {mean_VIP_room}, Food court= {mean_VIP_food}, Shopping mall= {mean_VIP_Mall}, Spa= {mean_VIP_Spa}, VR deck {mean_VIP_VR}")
print(f"No VIP means: Room service= {mean_nVIP_room}, Food court= {mean_nVIP_food}, Shopping mall= {mean_nVIP_Mall}, Spa= {mean_nVIP_Spa}, VR deck {mean_nVIP_VR}")

#filling NA with their means
train_data.loc[train_data['VIP']== False, 'RoomService']= train_data.loc[train_data['VIP']== False, 'RoomService'].fillna(mean_nVIP_room)
train_data.loc[train_data['VIP']== False, 'FoodCourt']= train_data.loc[train_data['VIP']== False, 'FoodCourt'].fillna(mean_nVIP_food)
train_data.loc[train_data['VIP']== False, 'ShoppingMall']= train_data.loc[train_data['VIP']== False, 'ShoppingMall'].fillna(mean_nVIP_Mall)
train_data.loc[train_data['VIP']== False, 'Spa']= train_data.loc[train_data['VIP']== False, 'Spa'].fillna(mean_nVIP_Spa)
train_data.loc[train_data['VIP']== False, 'VRDeck']= train_data.loc[train_data['VIP']== False, 'VRDeck'].fillna(mean_nVIP_VR)

train_data.loc[train_data['VIP']== True, 'RoomService']= train_data.loc[train_data['VIP']== True, 'RoomService'].fillna(mean_VIP_room)
train_data.loc[train_data['VIP']== True, 'FoodCourt']= train_data.loc[train_data['VIP']== True, 'FoodCourt'].fillna(mean_VIP_food)
train_data.loc[train_data['VIP']== True, 'ShoppingMall']= train_data.loc[train_data['VIP']== True, 'ShoppingMall'].fillna(mean_VIP_Mall)
train_data.loc[train_data['VIP']== True, 'Spa']= train_data.loc[train_data['VIP']== True, 'Spa'].fillna(mean_VIP_Spa)
train_data.loc[train_data['VIP']== True, 'VRDeck']= train_data.loc[train_data['VIP']== True, 'VRDeck'].fillna(mean_VIP_VR)

df_v5 = train_data.copy()

# Define a function to fill missing values based on groupby and mode
def fill_missing_by_group(df, column, group_col):
    df[column] = df.groupby(group_col)[column].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))
    return df

# Define a function to fill missing values based on lastname mode
def fill_missing_by_lastname(df, column, lastname_col):
    ln_gb = df.groupby(lastname_col)[column].agg(pd.Series.mode)
    ln_index = df[df[column].isna() & df[lastname_col].notna() & df[lastname_col].isin(ln_gb.index)].index

    def fill_mode(last_name):
        mode_value = ln_gb[last_name]
        if isinstance(mode_value, np.ndarray):
            return mode_value[0] if mode_value.size > 0 else np.nan
        return mode_value

    df.loc[ln_index, column] = df.loc[ln_index, lastname_col].map(fill_mode)
    return df

# Print the number of missing values before data cleaning
print("Number of missing values before data cleaning:")
for column in ['Cabin_side', 'Cabin_deck', 'Cabin_num']:
    mv_raw = df_v5[column].isna().sum()
    print(f'{column}: {mv_raw}')

# Apply the functions to fill missing values for each column
columns = ['Cabin_side', 'Cabin_deck', 'Cabin_num']
for column in columns:
    df_v5 = fill_missing_by_group(df_v5, column, 'Pgroup')
    df_v5 = fill_missing_by_lastname(df_v5, column, 'LastName')
    df_v5[column] = df_v5[column].fillna(df_v5[column].mode()[0])

# Print the number of missing values after data cleaning
print("Number of missing values after data cleaning:")
for column in ['Cabin_side', 'Cabin_deck', 'Cabin_num']:
    mv_final = df_v5[column].isna().sum()
    print(f'{column}: {mv_final}')

train_data.head()

train_data=df_v5
train_data.drop(columns=['Name','LastName','Expenditure'], inplace= True)
temp_data= train_data

#sklearn models can only use numerical data so we'll adapt the data
label_encoder= LabelEncoder()
train_data['HomePla_label']=label_encoder.fit_transform(train_data['HomePlanet'])
train_data['CryoSleep_L']=label_encoder.fit_transform(train_data['CryoSleep'])
train_data['Destination_L']=label_encoder.fit_transform(train_data['Destination'])
train_data['VIP_L']=label_encoder.fit_transform(train_data['VIP'])
train_data['Cabin_deck_L']=label_encoder.fit_transform(train_data['Cabin_deck'])
train_data['Cabin_side_L']=label_encoder.fit_transform(train_data['Cabin_side'])

def transported_cat(transported):
  if pd.isna(transported):
    return np.nan
  elif transported== True:
      return 1
  elif transported== False:
      return 0
train_data['Transported']=train_data['Transported'].apply(lambda x: transported_cat(x))
train_data.drop(columns=['HomePlanet','CryoSleep','Destination','VIP','Cabin_deck','Cabin_side'], inplace=True)
train_data.head()

train_data.info()

age_mode= train_data['Age'].mode()[0]
train_data['Age'].fillna(age_mode, inplace= True)

test_data = train_data.iloc[8693:].copy()
train_data= train_data.dropna(subset=(['Age','Transported']))
temp_data = train_data.iloc[:8693].copy()

transported = temp_data['Transported'].copy()
temp_data.drop(columns=['Transported'], inplace=True)
test_data.drop(columns=['Transported'], inplace=True)
transported.head()

import missingno as msno
msno.matrix(temp_data)



test_data.info()

transported.info()

temp_data.info()

#I'm going to do a few tests of diferents ways of separating data to check which is more acurate
# 80/20, 70/30, 60/20/20 and in each of them diferent sizes of a forest
test_sizes= [0.20,0.30,0.40]
forest_sizes= [55,56,57,58,59,60,61,62,63,64,65]
b=0
best=0
for i in test_sizes:
  train_X,test_X,train_Y,test_Y= train_test_split(temp_data,transported, test_size= i, random_state= 44)
  b +=1
  #if b == 2:
  #  test_X,val_X,test_Y,val_Y= train_test_split(test_X,test_Y, test_size= 0.5, random_state=44)
  for y in forest_sizes:
    tree_model= DecisionTreeClassifier(max_leaf_nodes=y, random_state= 44)
    tree_model.fit(train_X, train_Y)
    y_pred= tree_model.predict(test_X)
    #Display the information about each of the model created
    print(f'Confusion matrix of test size {i} and {y} leafs \n')
    print(confusion_matrix(test_Y,y_pred, labels=[True, False]))
    print('Accuracy score: ',round(accuracy_score(test_Y,y_pred), 4))
    print(classification_report(test_Y,y_pred))
    a=round(accuracy_score(test_Y,y_pred), 4)
    if a>best:
      best=a
      b_size=i
      b_leaf=y

print("_____________________________________________________________________________")
print(f'The best accuracy is {best}, obtained by test size {b_size} and {b_leaf} leaves')
print("_____________________________________________________________________________")

DTC= DecisionTreeClassifier(random_state= 44)
print(DTC.get_params())
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_leaf_nodes': [10, 40, 50,60],  #checked 20,30,55
    'max_depth': [18,19, 17, None], #checked: 5,10,15,16,20
    'min_impurity_decrease': [0.01, 0.1, 0.05, 0],  #checked 0, 0.01, 0.1, 0.2
    'min_samples_split': [2,3,4],   #checked 1,5,7
    'min_samples_leaf': [1, 2, 5],
    'min_weight_fraction_leaf': [0.01, 0.05, 0.0],  #checked 0, 0.01, 0.1, 0.2
    'max_features': ['sqrt', 'log2', None],
}

train_X,test_X,train_Y,test_Y= train_test_split(temp_data,transported, test_size= 0.2, random_state= 44)

grid_fitting= GridSearchCV(DTC, param_grid, cv=3, scoring= 'accuracy', n_jobs= 1)
grid_fitting.fit(train_X, train_Y)

# Best parameters
best_params = grid_fitting.best_params_
print(f"Best parameters: {best_params}")

# Train the model with best parameters
best_dtc = grid_fitting.best_estimator_
best_dtc.fit(train_X, train_Y)

dtc_y_pred= best_dtc.predict(test_X)
#Display the information about each of the model created
print(f'Confusion matrix \n')
print(confusion_matrix(test_Y,dtc_y_pred, labels=[True, False]))
print('Accuracy score: ',round(accuracy_score(test_Y,dtc_y_pred), 4))

y_probs = best_dtc.predict_proba(test_X)[:, 1]  # Get probabilities for the positive class
fpr, tpr, thresholds = roc_curve(test_Y, y_probs)


optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]


y_pred = (y_probs >= optimal_threshold).astype(int)

roc_auc = roc_auc_score(test_Y, y_probs)

# Plotting the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

RFC= RandomForestClassifier(random_state= 44)
print(RFC.get_params())
param_grid_rf = {
    'n_estimators': [450,375,400], #checked: 100,150,180,200,300,250,370,400,500
    'max_depth': [None,27,25],  #checked: None, 20, 30,35
    'min_samples_split': [3,2,5],  #checked: 2,5,10
    'min_samples_leaf': [4,5,2],  #checked: 1,2,4
    'max_features': ['sqrt'], #checked: None, sqrt, log2
    'bootstrap': [True, False],
    'criterion': ['gini', 'entropy'],
}

grid_fitting_rf= GridSearchCV(RFC, param_grid_rf, cv=7, scoring= 'accuracy', n_jobs= 1)
grid_fitting_rf.fit(train_X, train_Y)

# Best parameters
best_params_rf = grid_fitting_rf.best_params_
print(f"Best parameters: {best_params_rf}")


# Train the model with best parameters
best_rfc = grid_fitting_rf.best_estimator_
best_rfc.fit(train_X, train_Y)

rfc_y_pred= best_rfc.predict(test_X)
#Display the information about each of the model created
print(f'Confusion matrix \n')
print(confusion_matrix(test_Y,rfc_y_pred, labels=[True, False]))
print('Accuracy score: ',round(accuracy_score(test_Y,rfc_y_pred), 4))
print(classification_report(test_Y,rfc_y_pred))

"""Best parameters: {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 25, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 3, 'n_estimators': 400}
Confusion matrix

[[718 181]
 [177 663]]
Accuracy score:  0.7941
              precision    recall  f1-score   support

         0.0       0.79      0.79      0.79       840
         1.0       0.80      0.80      0.80       899

    accuracy                           0.79      1739
   macro avg       0.79      0.79      0.79      1739
weighted avg       0.79      0.79      0.79      1739
"""

y_probs_rf = best_rfc.predict_proba(test_X)[:, 1]  # Get probabilities for the positive class
fpr, tpr, thresholds = roc_curve(test_Y, y_probs_rf)


optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]


y_pred_rf = (y_probs_rf >= optimal_threshold).astype(int)
roc_auc = roc_auc_score(test_Y, y_probs_rf)

# Plotting the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

msno.matrix(test_data)

final= best_rfc.predict(test_data)



#final= best_rfc.predict(test_data)
submission = pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')
submission["Transported"] = final.astype(bool)

submission.to_csv('submission.csv', index=False)